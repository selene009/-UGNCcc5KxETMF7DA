{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **MonReader**","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-09-05T16:35:10.918014Z","iopub.execute_input":"2022-09-05T16:35:10.918406Z","iopub.status.idle":"2022-09-05T16:35:12.695884Z","shell.execute_reply.started":"2022-09-05T16:35:10.918358Z","shell.execute_reply":"2022-09-05T16:35:12.694788Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np\nimport torch\nfrom torch import nn\nimport torch.optim as optim\nimport torchvision\nfrom torchvision import transforms, models, datasets\nimport imageio\nimport time\nimport warnings\nimport random\nimport sys\nimport copy\nimport json\nfrom PIL import Image\n","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:35:12.697814Z","iopub.execute_input":"2022-09-05T16:35:12.698417Z","iopub.status.idle":"2022-09-05T16:35:13.589500Z","shell.execute_reply.started":"2022-09-05T16:35:12.698377Z","shell.execute_reply":"2022-09-05T16:35:13.588503Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"data_dir = '../input/flip-or-not-flip/images'\ntrain_dir = data_dir + '/training'\ntest_dir = data_dir + '/testing'","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:35:13.591069Z","iopub.execute_input":"2022-09-05T16:35:13.591641Z","iopub.status.idle":"2022-09-05T16:35:13.596610Z","shell.execute_reply.started":"2022-09-05T16:35:13.591599Z","shell.execute_reply":"2022-09-05T16:35:13.595606Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# image preparation\ndata_transforms = {\n    #data agument\n    'training': transforms.Compose([transforms.RandomRotation(45),#random rotation from -45 to 45 degree\n        transforms.CenterCrop(224),#cut from center 224 for VGG,Resnet\n        transforms.RandomHorizontalFlip(p=0.5),# random horizontal flip choosing one probabilty \n        transforms.RandomVerticalFlip(p=0.5),#random vertical flip \n        transforms.ColorJitter(brightness=0.2, contrast=0.1, saturation=0.1, hue=0.1),#brightness,contrast,saturation and hue\n        transforms.RandomGrayscale(p=0.025),#change to grayscale probability \n        transforms.ToTensor(),#data change to tensor\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])# normalize AS vgg using imagenet\n    ]),\n    # only need test, no need data agument\n    'testing': transforms.Compose([transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])#must do the same thing as training data\n    ]),\n}","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:35:13.598934Z","iopub.execute_input":"2022-09-05T16:35:13.599551Z","iopub.status.idle":"2022-09-05T16:35:13.608813Z","shell.execute_reply.started":"2022-09-05T16:35:13.599515Z","shell.execute_reply":"2022-09-05T16:35:13.607748Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"batch_size = 8\n\nimage_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['training', 'testing']}\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True) for x in ['training', 'testing']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['training', 'testing']}\nclass_names = image_datasets['training'].classes","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:35:13.610254Z","iopub.execute_input":"2022-09-05T16:35:13.610603Z","iopub.status.idle":"2022-09-05T16:35:13.640656Z","shell.execute_reply.started":"2022-09-05T16:35:13.610570Z","shell.execute_reply":"2022-09-05T16:35:13.639839Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"image_datasets","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:35:13.642795Z","iopub.execute_input":"2022-09-05T16:35:13.643366Z","iopub.status.idle":"2022-09-05T16:35:13.653525Z","shell.execute_reply.started":"2022-09-05T16:35:13.643332Z","shell.execute_reply":"2022-09-05T16:35:13.652258Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class_names","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:35:13.655502Z","iopub.execute_input":"2022-09-05T16:35:13.656047Z","iopub.status.idle":"2022-09-05T16:35:13.669747Z","shell.execute_reply.started":"2022-09-05T16:35:13.656012Z","shell.execute_reply":"2022-09-05T16:35:13.662537Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"**show images**","metadata":{}},{"cell_type":"code","source":"def im_convert(tensor):\n    \"\"\" 展示数据\"\"\"\n    \n    image = tensor.to(\"cpu\").clone().detach()\n    image = image.numpy().squeeze()\n    image = image.transpose(1,2,0)\n    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n    image = image.clip(0, 1)\n\n    return image","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:35:13.921204Z","iopub.execute_input":"2022-09-05T16:35:13.921915Z","iopub.status.idle":"2022-09-05T16:35:13.928332Z","shell.execute_reply.started":"2022-09-05T16:35:13.921879Z","shell.execute_reply":"2022-09-05T16:35:13.927136Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"fig=plt.figure(figsize=(20, 12))\ncolumns = 4\nrows = 2\n\ndataiter = iter(dataloaders['training'])\ninputs, classes = dataiter.next()\n\nfor idx in range (columns*rows):\n    ax = fig.add_subplot(rows, columns, idx+1, xticks=[], yticks=[])\n    ax.set_title(class_names[classes[idx]])\n    plt.imshow(im_convert(inputs[idx]))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:35:14.247251Z","iopub.execute_input":"2022-09-05T16:35:14.250696Z","iopub.status.idle":"2022-09-05T16:35:15.628265Z","shell.execute_reply.started":"2022-09-05T16:35:14.250642Z","shell.execute_reply":"2022-09-05T16:35:15.627396Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"**choose resnet as model use trained weight as intial parameters**","metadata":{}},{"cell_type":"code","source":"model_name = 'resnet'  # ['resnet', 'alexnet', 'vgg', 'squeezenet', 'densenet', 'inception']\n# set trained feature\nfeature_extract = True ","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:35:15.629896Z","iopub.execute_input":"2022-09-05T16:35:15.630416Z","iopub.status.idle":"2022-09-05T16:35:15.635681Z","shell.execute_reply.started":"2022-09-05T16:35:15.630382Z","shell.execute_reply":"2022-09-05T16:35:15.634297Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# set GPU \ntrain_on_gpu = torch.cuda.is_available()\n\nif not train_on_gpu:\n    print('CUDA is not available.  Training on CPU ...')\nelse:\n    print('CUDA is available!  Training on GPU ...')\n    \ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:35:15.637422Z","iopub.execute_input":"2022-09-05T16:35:15.638421Z","iopub.status.idle":"2022-09-05T16:35:15.709218Z","shell.execute_reply.started":"2022-09-05T16:35:15.638388Z","shell.execute_reply":"2022-09-05T16:35:15.708071Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# if freeze all the layer from reset \ndef set_parameter_requires_grad(model, feature_extracting):\n    if feature_extracting:\n        for param in model.parameters():\n            param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:35:15.712596Z","iopub.execute_input":"2022-09-05T16:35:15.713363Z","iopub.status.idle":"2022-09-05T16:35:15.718895Z","shell.execute_reply.started":"2022-09-05T16:35:15.713323Z","shell.execute_reply":"2022-09-05T16:35:15.717876Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"**load resnet model**","metadata":{}},{"cell_type":"code","source":"model_ft = models.resnet152()\nmodel_ft","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:35:16.301433Z","iopub.execute_input":"2022-09-05T16:35:16.302140Z","iopub.status.idle":"2022-09-05T16:35:17.379385Z","shell.execute_reply.started":"2022-09-05T16:35:16.302103Z","shell.execute_reply":"2022-09-05T16:35:17.378491Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"**reference PyTorch offical website example**","metadata":{}},{"cell_type":"code","source":"def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n    # 选择合适的模型，不同模型的初始化方法稍微有点区别\n    model_ft = None\n    input_size = 0\n\n    if model_name == \"resnet\":\n        \"\"\" Resnet152\n        \"\"\"\n        model_ft = models.resnet152(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.fc.in_features\n        model_ft.fc = nn.Sequential(nn.Linear(num_ftrs, 102),\n                                   nn.LogSoftmax(dim=1))\n        input_size = 224\n\n    elif model_name == \"alexnet\":\n        \"\"\" Alexnet\n        \"\"\"\n        model_ft = models.alexnet(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.classifier[6].in_features\n        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n        input_size = 224\n\n    elif model_name == \"vgg\":\n        \"\"\" VGG11_bn\n        \"\"\"\n        model_ft = models.vgg16(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.classifier[6].in_features\n        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n        input_size = 224\n\n    elif model_name == \"squeezenet\":\n        \"\"\" Squeezenet\n        \"\"\"\n        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n        model_ft.num_classes = num_classes\n        input_size = 224\n\n    elif model_name == \"densenet\":\n        \"\"\" Densenet\n        \"\"\"\n        model_ft = models.densenet121(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.classifier.in_features\n        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n        input_size = 224\n\n    elif model_name == \"inception\":\n        \"\"\" Inception v3\n        Be careful, expects (299,299) sized images and has auxiliary output\n        \"\"\"\n        model_ft = models.inception_v3(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        # Handle the auxilary net\n        num_ftrs = model_ft.AuxLogits.fc.in_features\n        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n        # Handle the primary net\n        num_ftrs = model_ft.fc.in_features\n        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n        input_size = 299\n\n    else:\n        print(\"Invalid model name, exiting...\")\n        exit()\n\n    return model_ft, input_size","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:35:17.381619Z","iopub.execute_input":"2022-09-05T16:35:17.382034Z","iopub.status.idle":"2022-09-05T16:35:17.394669Z","shell.execute_reply.started":"2022-09-05T16:35:17.381982Z","shell.execute_reply":"2022-09-05T16:35:17.393456Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"model_ft, input_size = initialize_model(model_name, 102, feature_extract, use_pretrained=True)\n\n#GPU计算\nmodel_ft = model_ft.to(device)\n\n# 模型保存\nfilename='checkpoint.pth'\n\n# 是否训练所有层\nparams_to_update = model_ft.parameters()\nprint(\"Params to learn:\")\nif feature_extract:\n    params_to_update = []\n    for name,param in model_ft.named_parameters():\n        if param.requires_grad == True:\n            params_to_update.append(param)\n            print(\"\\t\",name)\nelse:\n    for name,param in model_ft.named_parameters():\n        if param.requires_grad == True:\n            print(\"\\t\",name)","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:35:17.396665Z","iopub.execute_input":"2022-09-05T16:35:17.397204Z","iopub.status.idle":"2022-09-05T16:35:44.324167Z","shell.execute_reply.started":"2022-09-05T16:35:17.397123Z","shell.execute_reply":"2022-09-05T16:35:44.323079Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"model_ft","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:35:44.326457Z","iopub.execute_input":"2022-09-05T16:35:44.327017Z","iopub.status.idle":"2022-09-05T16:35:44.339075Z","shell.execute_reply.started":"2022-09-05T16:35:44.326953Z","shell.execute_reply":"2022-09-05T16:35:44.337010Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# set optimizer\noptimizer_ft = optim.Adam(params_to_update, lr=1e-2)\nscheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)#lr decrease per 7 to 1/10\n\n# as the last layer using LogSoftmax(), cannot apply nn.CrossEntropyLoss(),nn.CrossEntropyLoss() is the result of combining logSoftmax() and nn.NLLLoss()\ncriterion = nn.NLLLoss()","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:35:44.340597Z","iopub.execute_input":"2022-09-05T16:35:44.342571Z","iopub.status.idle":"2022-09-05T16:35:44.349453Z","shell.execute_reply.started":"2022-09-05T16:35:44.342534Z","shell.execute_reply":"2022-09-05T16:35:44.348482Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# set training model ","metadata":{}},{"cell_type":"code","source":"def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False,filename=filename):\n    since = time.time()\n    best_acc = 0\n    \"\"\"\n    checkpoint = torch.load(filename)\n    best_acc = checkpoint['best_acc']\n    model.load_state_dict(checkpoint['state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer'])\n    model.class_to_idx = checkpoint['mapping']\n    \"\"\"\n    model.to(device)\n\n    val_acc_history = []\n    train_acc_history = []\n    train_losses = []\n    valid_losses = []\n    LRs = [optimizer.param_groups[0]['lr']]\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        # 训练和验证\n        for phase in ['training', 'testing']:\n            if phase == 'training':\n                model.train()  # 训练\n            else:\n                model.eval()   # 验证\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # 把数据都取个遍\n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # 清零\n                optimizer.zero_grad()\n                # 只有训练的时候计算和更新梯度\n                with torch.set_grad_enabled(phase == 'training'):\n                    if is_inception and phase == 'training':\n                        outputs, aux_outputs = model(inputs)\n                        loss1 = criterion(outputs, labels)\n                        loss2 = criterion(aux_outputs, labels)\n                        loss = loss1 + 0.4*loss2\n                    else:#resnet执行的是这里\n                        outputs = model(inputs)\n                        loss = criterion(outputs, labels)\n\n                    _, preds = torch.max(outputs, 1)\n\n                    # 训练阶段更新权重\n                    if phase == 'training':\n                        loss.backward()\n                        optimizer.step()\n\n                # 计算损失\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n            \n            \n            time_elapsed = time.time() - since\n            print('Time elapsed {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n            \n\n            # save the best model for each epoch \n            if phase == 'testing' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n                #save the model \n                state = {\n                  'state_dict': model.state_dict(),\n                  'best_acc': best_acc,\n                  'optimizer' : optimizer.state_dict(),\n                }\n                torch.save(state, filename)\n            if phase == 'testing':\n                val_acc_history.append(epoch_acc)\n                valid_losses.append(epoch_loss)\n                scheduler.step(epoch_loss)\n            if phase == 'training':\n                train_acc_history.append(epoch_acc)\n                train_losses.append(epoch_loss)\n        \n        print('Optimizer learning rate : {:.7f}'.format(optimizer.param_groups[0]['lr']))\n        LRs.append(optimizer.param_groups[0]['lr'])\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n    print('Best test Acc: {:4f}'.format(best_acc))\n\n    # save best training model result a\n    model.load_state_dict(best_model_wts)\n    return model, val_acc_history, train_acc_history, valid_losses, train_losses, LRs ","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:35:44.352609Z","iopub.execute_input":"2022-09-05T16:35:44.353432Z","iopub.status.idle":"2022-09-05T16:35:44.370282Z","shell.execute_reply.started":"2022-09-05T16:35:44.353399Z","shell.execute_reply":"2022-09-05T16:35:44.369321Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# Train model ","metadata":{}},{"cell_type":"code","source":"model_ft, val_acc_history, train_acc_history, valid_losses, train_losses, LRs  = train_model(model_ft, dataloaders, criterion, optimizer_ft, num_epochs=10, is_inception=(model_name==\"inception\"))","metadata":{"execution":{"iopub.status.busy":"2022-09-05T16:35:44.373378Z","iopub.execute_input":"2022-09-05T16:35:44.373709Z","iopub.status.idle":"2022-09-05T17:00:39.160877Z","shell.execute_reply.started":"2022-09-05T16:35:44.373684Z","shell.execute_reply":"2022-09-05T17:00:39.159728Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# **Training all the layers as the result of previous  last layer training is good**","metadata":{}},{"cell_type":"code","source":"for param in model_ft.parameters():\n    param.requires_grad = True\n\n# 再继续训练所有的参数，学习率调小一点\noptimizer = optim.Adam(params_to_update, lr=1e-4)\nscheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n\n# 损失函数\ncriterion = nn.NLLLoss()","metadata":{"execution":{"iopub.status.busy":"2022-09-05T17:00:39.164178Z","iopub.execute_input":"2022-09-05T17:00:39.164464Z","iopub.status.idle":"2022-09-05T17:00:39.171889Z","shell.execute_reply.started":"2022-09-05T17:00:39.164438Z","shell.execute_reply":"2022-09-05T17:00:39.170631Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Load the checkpoint\n\ncheckpoint = torch.load(filename)\nbest_acc = checkpoint['best_acc']\nmodel_ft.load_state_dict(checkpoint['state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer'])\n#model_ft.class_to_idx = checkpoint['mapping']","metadata":{"execution":{"iopub.status.busy":"2022-09-05T17:00:39.173728Z","iopub.execute_input":"2022-09-05T17:00:39.175916Z","iopub.status.idle":"2022-09-05T17:00:39.514145Z","shell.execute_reply.started":"2022-09-05T17:00:39.175887Z","shell.execute_reply":"2022-09-05T17:00:39.513174Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"model_ft, val_acc_history, train_acc_history, valid_losses, train_losses, LRs  = train_model(model_ft, dataloaders, criterion, optimizer, num_epochs=5, is_inception=(model_name==\"inception\"))","metadata":{"execution":{"iopub.status.busy":"2022-09-05T17:00:39.515419Z","iopub.execute_input":"2022-09-05T17:00:39.517549Z","iopub.status.idle":"2022-09-05T17:14:55.795803Z","shell.execute_reply.started":"2022-09-05T17:00:39.517520Z","shell.execute_reply":"2022-09-05T17:14:55.794743Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# **Load Trained model**","metadata":{}},{"cell_type":"code","source":"# 得到一个batch的测试数据\ndataiter = iter(dataloaders['testing'])\nimages, labels = dataiter.next()\n\nmodel_ft.eval()\n\nif train_on_gpu:\n    output = model_ft(images.cuda())\nelse:\n    output = model_ft(images)","metadata":{"execution":{"iopub.status.busy":"2022-09-05T17:14:55.798202Z","iopub.execute_input":"2022-09-05T17:14:55.798875Z","iopub.status.idle":"2022-09-05T17:14:56.316277Z","shell.execute_reply.started":"2022-09-05T17:14:55.798838Z","shell.execute_reply":"2022-09-05T17:14:56.315047Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"labels","metadata":{"execution":{"iopub.status.busy":"2022-09-05T17:17:03.782687Z","iopub.execute_input":"2022-09-05T17:17:03.783130Z","iopub.status.idle":"2022-09-05T17:17:03.791316Z","shell.execute_reply.started":"2022-09-05T17:17:03.783094Z","shell.execute_reply":"2022-09-05T17:17:03.790259Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"_, preds_tensor = torch.max(output, 1)\n\npreds = np.squeeze(preds_tensor.numpy()) if not train_on_gpu else np.squeeze(preds_tensor.cpu().numpy())\npreds","metadata":{"execution":{"iopub.status.busy":"2022-09-05T17:17:39.574839Z","iopub.execute_input":"2022-09-05T17:17:39.575447Z","iopub.status.idle":"2022-09-05T17:17:39.585637Z","shell.execute_reply.started":"2022-09-05T17:17:39.575412Z","shell.execute_reply":"2022-09-05T17:17:39.584133Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"# **F1 SCORE**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import f1_score                  \nf1_score(preds,labels, average='micro')          \n","metadata":{"execution":{"iopub.status.busy":"2022-09-05T17:19:15.613576Z","iopub.execute_input":"2022-09-05T17:19:15.613955Z","iopub.status.idle":"2022-09-05T17:19:16.017711Z","shell.execute_reply.started":"2022-09-05T17:19:15.613922Z","shell.execute_reply":"2022-09-05T17:19:16.016742Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}