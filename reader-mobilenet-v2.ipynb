{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-09-13T14:13:45.677980Z","iopub.execute_input":"2022-09-13T14:13:45.678451Z","iopub.status.idle":"2022-09-13T14:13:46.119597Z","shell.execute_reply.started":"2022-09-13T14:13:45.678409Z","shell.execute_reply":"2022-09-13T14:13:46.118364Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"originaldata_dir = '../input/flip-or-not-flip/images'\ntrain_dir = originaldata_dir + '/training'\ntest_dir = originaldata_dir + '/testing'","metadata":{"execution":{"iopub.status.busy":"2022-09-13T14:15:59.936512Z","iopub.execute_input":"2022-09-13T14:15:59.936984Z","iopub.status.idle":"2022-09-13T14:15:59.942312Z","shell.execute_reply.started":"2022-09-13T14:15:59.936940Z","shell.execute_reply":"2022-09-13T14:15:59.941152Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Split train data and validation data","metadata":{}},{"cell_type":"code","source":"\nimport os\nimport shutil\nfrom sklearn.model_selection import train_test_split\n\n# 创建文件夹\ndef mkdir(path):\n    folder = os.path.exists(path)\n    if not folder:\n        os.makedirs(path)\n        print(f'-- new folder \"{path}\" --')\n    else:\n        print(f'-- the folder \"{path}\" is already here --')\n# read the flip and notflip folds of original training data\ndataset_path_flip = \"../input/flip-or-not-flip/images/training/flip\"\ndataset_path_notflip = \"../input/flip-or-not-flip/images/training/notflip\"\n\n# create the  save path of flip and notflip folds on training and validation data\ntrain_set_save_path_flip = \"./training/flip\"\ntrain_set_save_path_notflip = \"./training/notflip\"\nval_set_save_path_flip = \"./validation/flip\"\nval_set_save_path_notflip = \"./validation/notflip\"\n\n# save the path \nmkdir(train_set_save_path_flip)\nmkdir(train_set_save_path_notflip)\nmkdir(val_set_save_path_flip)\nmkdir(val_set_save_path_notflip)\n\n#read the original flip and notflip folds of original training data \n\nfile_pathes_flip = os.listdir(dataset_path_flip)\nfile_pathes_notflip = os.listdir(dataset_path_notflip)\n# print(file_pathes_flip)\n\n\n# 获取training文件夹下所有flip图像的名称（不包含后缀名）\nimg_names_flip = []\nfor file_path in file_pathes_flip:\n    if os.path.splitext(file_path)[1] == \".jpg\":\n        file_name = os.path.splitext(file_path)[0]\n        img_names_flip.append(file_name)\n\n# 获取training文件夹下所有notflip图像的名称（不包含后缀名）\nimg_names_notflip = []\nfor file_path in file_pathes_notflip:\n    if os.path.splitext(file_path)[1] == \".jpg\":\n        file_name = os.path.splitext(file_path)[0]\n        img_names_notflip.append(file_name)\n\n\n        \n        \n# split the test and validation data from flip and notflip folds\ntrain_set_flip, val_set_flip = train_test_split(img_names_flip, test_size=0.2, random_state=42)\ntrain_set_notflip, val_set_notflip = train_test_split(img_names_notflip, test_size=0.2, random_state=42)\n\n# print(f\"train_set size: {len(train_set)}, val_set size: {len(val_set)}\")\n\n\n# train处理：将train_set_flip移动到目标文件夹\nfor file_name in train_set_flip:\n    train_img_src_path_flip = os.path.join(dataset_path_flip, file_name+\".jpg\")\n    train_img_dst_path_flip = os.path.join(train_set_save_path_flip, file_name+\".jpg\")\n    shutil.copyfile(train_img_src_path_flip,train_img_dst_path_flip)\n\n# train处理：将train_set_notflip移动到目标文件夹\nfor file_name in train_set_notflip:\n    train_img_src_path_notflip = os.path.join(dataset_path_notflip, file_name+\".jpg\")\n    train_img_dst_path_notflip = os.path.join(train_set_save_path_notflip, file_name+\".jpg\")\n    shutil.copyfile(train_img_src_path_notflip,train_img_dst_path_notflip)\n    \n# validation处理：将val_set_flip移动到目标文件夹\nfor file_name in val_set_flip:\n    val_img_src_path_flip = os.path.join(dataset_path_flip, file_name+\".jpg\")\n    val_img_dst_path_flip = os.path.join(val_set_save_path_flip, file_name+\".jpg\")\n    shutil.copyfile(val_img_src_path_flip,val_img_dst_path_flip)   \n\n# validation处理：将val_set_flip移动到目标文件夹\nfor file_name in val_set_notflip:\n    val_img_src_path_notflip = os.path.join(dataset_path_notflip, file_name+\".jpg\")\n    val_img_dst_path_notflip = os.path.join(val_set_save_path_notflip, file_name+\".jpg\")\n    shutil.copyfile(val_img_src_path_notflip,val_img_dst_path_notflip)   \n \n","metadata":{"execution":{"iopub.status.busy":"2022-09-13T14:16:02.876585Z","iopub.execute_input":"2022-09-13T14:16:02.877047Z","iopub.status.idle":"2022-09-13T14:16:02.887675Z","shell.execute_reply.started":"2022-09-13T14:16:02.877011Z","shell.execute_reply":"2022-09-13T14:16:02.886301Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# set model","metadata":{}},{"cell_type":"code","source":"from torch import nn\nimport torch\ndef _make_divisible(ch, divisor=8, min_ch=None):\n    \"\"\"\n    This function is taken from the original tf repo.\n    It ensures that all layers have a channel number that is divisible by 8\n    It can be seen here:\n    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n    \"\"\"\n    if min_ch is None:\n        min_ch = divisor\n    new_ch = max(min_ch, int(ch + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_ch < 0.9 * ch:\n        new_ch += divisor\n    return new_ch\n\n\nclass ConvBNReLU(nn.Sequential):\n    def __init__(self, in_channel, out_channel, kernel_size=3, stride=1, groups=1):\n        padding = (kernel_size - 1) // 2\n        super(ConvBNReLU, self).__init__(\n            nn.Conv2d(in_channel, out_channel, kernel_size, stride, padding, groups=groups, bias=False),\n            nn.BatchNorm2d(out_channel),\n            nn.ReLU6(inplace=True)\n        )\n\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, in_channel, out_channel, stride, expand_ratio):\n        super(InvertedResidual, self).__init__()\n        hidden_channel = in_channel * expand_ratio\n        self.use_shortcut = stride == 1 and in_channel == out_channel\n\n        layers = []\n        if expand_ratio != 1:\n            # 1x1 pointwise conv\n            layers.append(ConvBNReLU(in_channel, hidden_channel, kernel_size=1))\n        layers.extend([\n            # 3x3 depthwise conv\n            ConvBNReLU(hidden_channel, hidden_channel, stride=stride, groups=hidden_channel),\n            # 1x1 pointwise conv(linear)\n            nn.Conv2d(hidden_channel, out_channel, kernel_size=1, bias=False),\n            nn.BatchNorm2d(out_channel),\n        ])\n\n        self.conv = nn.Sequential(*layers)\n\n    def forward(self, x):\n        if self.use_shortcut:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n\n\nclass MobileNetV2(nn.Module):\n    def __init__(self, num_classes=1000, alpha=1.0, round_nearest=8):\n        super(MobileNetV2, self).__init__()\n        block = InvertedResidual\n        input_channel = _make_divisible(32 * alpha, round_nearest)\n        last_channel = _make_divisible(1280 * alpha, round_nearest)\n\n        inverted_residual_setting = [\n            # t, c, n, s\n            [1, 16, 1, 1],\n            [6, 24, 2, 2],\n            [6, 32, 3, 2],\n            [6, 64, 4, 2],\n            [6, 96, 3, 1],\n            [6, 160, 3, 2],\n            [6, 320, 1, 1],\n        ]\n\n        features = []\n        # conv1 layer\n        features.append(ConvBNReLU(3, input_channel, stride=2))\n        # building inverted residual residual blockes\n        for t, c, n, s in inverted_residual_setting:\n            output_channel = _make_divisible(c * alpha, round_nearest)\n            for i in range(n):\n                stride = s if i == 0 else 1\n                features.append(block(input_channel, output_channel, stride, expand_ratio=t))\n                input_channel = output_channel\n        # building last several layers\n        features.append(ConvBNReLU(input_channel, last_channel, 1))\n        # combine feature layers\n        self.features = nn.Sequential(*features)\n\n        # building classifier\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.2),\n            nn.Linear(last_channel, num_classes)\n        )\n\n        # weight initialization\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-09-13T14:16:03.774042Z","iopub.execute_input":"2022-09-13T14:16:03.774815Z","iopub.status.idle":"2022-09-13T14:16:03.801243Z","shell.execute_reply.started":"2022-09-13T14:16:03.774771Z","shell.execute_reply":"2022-09-13T14:16:03.799167Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import os\nimport sys\nimport json\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import transforms, datasets\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2022-09-13T14:16:05.371267Z","iopub.execute_input":"2022-09-13T14:16:05.372052Z","iopub.status.idle":"2022-09-13T14:16:05.377867Z","shell.execute_reply.started":"2022-09-13T14:16:05.372010Z","shell.execute_reply":"2022-09-13T14:16:05.376984Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"data_dir = './'\ntrain_dir = data_dir + '/training'\nvalidation_dir = data_dir + '/validation'","metadata":{"execution":{"iopub.status.busy":"2022-09-13T14:37:20.576907Z","iopub.execute_input":"2022-09-13T14:37:20.577360Z","iopub.status.idle":"2022-09-13T14:37:20.583220Z","shell.execute_reply.started":"2022-09-13T14:37:20.577316Z","shell.execute_reply":"2022-09-13T14:37:20.581764Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# Train model","metadata":{}},{"cell_type":"code","source":"\n\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(\"using {} device.\".format(device))\n\nbatch_size = 16\nepochs = 10\n\ndata_transform = {\n        \"train\": transforms.Compose([transforms.RandomResizedCrop(224),\n                                     transforms.RandomHorizontalFlip(),\n                                     transforms.ToTensor(),\n                                     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n        \"validation\": transforms.Compose([transforms.Resize(256),\n                                   transforms.CenterCrop(224),\n                                   transforms.ToTensor(),\n                                   transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])}\n\ntrain_dataset = datasets.ImageFolder(root=os.path.join(train_dir),\n                                         transform=data_transform[\"train\"])\ntrain_num = len(train_dataset)\n\nnw = min([os.cpu_count(), batch_size if batch_size > 1 else 0, 8])  # number of workers\nprint('Using {} dataloader workers every process'.format(nw))\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset,\n                                               batch_size=batch_size, shuffle=True,\n                                               num_workers=nw)\n\nvalidation_dataset = datasets.ImageFolder(root=os.path.join(validation_dir),\n                                            transform=data_transform[\"validation\"])\nvalidation_num = len(validation_dataset)\nvalidation_loader = torch.utils.data.DataLoader(validation_dataset,\n                                                  batch_size=batch_size, shuffle=False,\n                                                  num_workers=nw)\n\nprint(\"using {} images for training, {} images for validation.\".format(train_num,\n                                                                           validation_num))\n\n# create model\nnet = MobileNetV2(num_classes=2)\n\n    \n# unfreeze features weights\nfor param in net.features.parameters():\n    param.requires_grad = True  \n\nnet.to(device)\n\n# define loss function\nloss_function = nn.CrossEntropyLoss()\n\n# construct an optimizer\nparams = [p for p in net.parameters() if p.requires_grad]\noptimizer = optim.Adam(params, lr=0.0001)\n\nbest_acc = 0.0\nsave_path = './MobileNetV2.pth'\ntrain_steps = len(train_loader)\nfor epoch in range(epochs):\n    # train\n    net.train()\n    running_loss = 0.0\n    train_bar = tqdm(train_loader, file=sys.stdout)\n    for step, data in enumerate(train_bar):\n        images, labels = data\n        optimizer.zero_grad()\n        logits = net(images.to(device))\n        loss = loss_function(logits, labels.to(device))\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n\n        train_bar.desc = \"train epoch[{}/{}] loss:{:.3f}\".format(epoch + 1,\n                                                                     epochs,\n                                                                     loss)\n\n    # validation\n    net.eval()\n    acc = 0.0  # accumulate accurate number / epoch\n    with torch.no_grad():\n        validation_bar = tqdm(validation_loader, file=sys.stdout)\n        for validation_data in validation_bar:\n            validation_images, validation_labels = validation_data\n            outputs = net(validation_images.to(device))\n            # loss = loss_function(outputs, test_labels)\n            predict_y = torch.max(outputs, dim=1)[1]\n            acc += torch.eq(predict_y, validation_labels.to(device)).sum().item()\n\n            validation_bar.desc = \"test epoch[{}/{}]\".format(epoch + 1,\n                                                           epochs)\n    validation_accurate = acc / validation_num\n    print('[epoch %d] train_loss: %.3f  validation_accuracy: %.3f' %\n              (epoch + 1, running_loss / train_steps, validation_accurate))\n\n    if validation_accurate > best_acc:\n        best_acc = validation_accurate\n        torch.save(net.state_dict(), save_path)\n\nprint('Finished Training')\nprint(predict_y)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-09-13T14:45:09.308820Z","iopub.execute_input":"2022-09-13T14:45:09.309260Z","iopub.status.idle":"2022-09-13T15:24:56.786991Z","shell.execute_reply.started":"2022-09-13T14:45:09.309205Z","shell.execute_reply":"2022-09-13T15:24:56.785430Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"originaldata_dir = '../input/flip-or-not-flip/images'\ntest_dir = originaldata_dir + '/testing'","metadata":{"execution":{"iopub.status.busy":"2022-09-13T15:35:51.321613Z","iopub.execute_input":"2022-09-13T15:35:51.323860Z","iopub.status.idle":"2022-09-13T15:35:51.331836Z","shell.execute_reply.started":"2022-09-13T15:35:51.323788Z","shell.execute_reply":"2022-09-13T15:35:51.330500Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"\"\"\"\ndata_transform = {\n        \"train\": transforms.Compose([transforms.RandomResizedCrop(224),\n                                     transforms.RandomHorizontalFlip(),\n                                     transforms.ToTensor(),\n                                     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n        \"test\": transforms.Compose([transforms.Resize(256),\n                                   transforms.CenterCrop(224),\n                                   transforms.ToTensor(),\n                                   transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])}\n\ntest_dataset = datasets.ImageFolder(root=os.path.join(test_dir),\n                                            transform=data_transform[\"test\"])\n\ntest_loader = torch.utils.data.DataLoader(test_dataset,\n                                                  batch_size=batch_size, shuffle=False,\n                                                  num_workers=nw)\n\n\nwith torch.no_grad():\n        test_bar = tqdm(test_loader, file=sys.stdout)\n        for test_data in test_bar:\n            test_images, test_labels = test_data\n            outputs = net(test_images.to(device))\n            # loss = loss_function(outputs, test_labels)\n            predict_y = torch.max(outputs, dim=1)[1]\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-09-13T15:35:53.408960Z","iopub.execute_input":"2022-09-13T15:35:53.409401Z","iopub.status.idle":"2022-09-13T15:36:19.828647Z","shell.execute_reply.started":"2022-09-13T15:35:53.409367Z","shell.execute_reply":"2022-09-13T15:36:19.827124Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"batch_size = 16\nepochs = 10","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test Model","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\ndata_transform = {\n        \n        \"test\": transforms.Compose([transforms.Resize(256),\n                                   transforms.CenterCrop(224),\n                                   transforms.ToTensor(),\n                                   transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])}\nbatch_size = 16\nepochs = 10\n                                \ntest_dataset = datasets.ImageFolder(root=os.path.join(test_dir),\n                                            transform=data_transform[\"test\"])\ntest_num = len(test_dataset)\ntest_loader = torch.utils.data.DataLoader(test_dataset,\n                                                  batch_size=batch_size, shuffle=False,\n                                          num_workers=nw)\n\n# create model\n\nnet.eval()\ntarget_num = torch.zeros((1, 2)) # n_classes为分类任务类别数量\npredict_num = torch.zeros((1, 2))\nacc_num = torch.zeros((1, 2))\nwith torch.no_grad():\n    test_bar = tqdm(test_loader, file=sys.stdout)\n    for test_data in test_bar:\n        test_images, test_labels = test_data\n        outputs = net(test_images.to(device))\n        # loss = loss_function(outputs, test_labels)\n        predict_y = torch.max(outputs, dim=1)[1]\n        pre_mask = torch.zeros(outputs.size()).scatter_(1, predict_y.cpu().view(-1, 1), 1.)\n        predict_num += pre_mask.sum(0)  # 得到数据中每类的预测量\n        print(predict_num)\n        tar_mask = torch.zeros(outputs.size()).scatter_(1, test_labels.data.cpu().view(-1, 1), 1.)\n        target_num += tar_mask.sum(0)  # 得到数据中每类的数量\n        acc_mask = pre_mask * tar_mask \n        acc_num += acc_mask.sum(0) # 得到各类别分类正确的样本数量\n    recall = acc_num / target_num\n    precision = acc_num / predict_num\n    F1 = 2 * recall * precision / (recall + precision)\n    \n    print('Test  F1-score {}'.format(F1))","metadata":{"execution":{"iopub.status.busy":"2022-09-13T16:39:56.136811Z","iopub.execute_input":"2022-09-13T16:39:56.137338Z","iopub.status.idle":"2022-09-13T16:40:20.991006Z","shell.execute_reply.started":"2022-09-13T16:39:56.137262Z","shell.execute_reply":"2022-09-13T16:40:20.989479Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}